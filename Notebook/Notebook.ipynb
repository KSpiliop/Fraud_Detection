{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning XGBoost hyper-parameters with heuristic search  \n",
    "# An example in Credit Card Fraud Detection\n",
    "\n",
    "\n",
    "The purpose of this experiment is to show how heuristics such as [Simulated Annealing](https://en.wikipedia.org/wiki/Simulated_annealing \"Simulated Annealing\") can be used to find efficiently good combinations of hyper-parameters in machine learning algorithms. This approach is better than blind random generation of parameters. It is also preferable to fine-tuning each hyper-parameter separately because typically there are interactions between them. \n",
    "\n",
    "The XGBoost algorithm is a good show case because it has many hyper-parameters. Exhaustive grid search can be computationally prohibitive.   \n",
    "\n",
    "For a very good discussion of the theoretical details of XGBoost, see this [Slideshare presentation](https://www.slideshare.net/ShangxuanZhang/kaggle-winning-solution-xgboost-algorithm-let-us-learn-from-its-author \"Slideshare presentation\") of the algorithm with title \"*Kaggle Winning Solution Xgboost algorithm -- Let us learn from its author*\" by Tianqi Chen.\n",
    "\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "This is a Kaggle dataset taken from [here](https://www.kaggle.com/dalpozz/creditcardfraud \"Kaggle dataset\") which contains credit card transactions data and a fraud flag. It appeared originally in [Dal Pozzolo, Andrea, et al. \"Calibrating Probability with Undersampling for Unbalanced Classification.\" Computational Intelligence, 2015 IEEE Symposium Series on. IEEE, 2015](http://www.oliviercaelen.be/doc/SSCI_calib_final.pdf \"Original paper\"). There is a Time variable (seconds from the first transaction in the dataset), an Amount variable, the Class variable (1=fraud, 0= no fraud) and the rest (V1-V28) are factor variables obtained through Principal Components Analysis from the original variables.\n",
    "\n",
    "This is not a very difficult case for XGBoost as it will be seen. The main objective in this experiment is to show that the heuristic search finds a suitable set of hyper-parameters out of a quite large set of potential combinations.\n",
    "\n",
    "We can verify below that this is a highly imbalanced dataset, typical of fraud detection data. We will take this into account when setting the weights of observations in XGBoost parameters. \n",
    "\n",
    "Since we have plenty of data we are going to calibrate the hyper-parameters on a validation dataset and evaluate performance on an unseen testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "random.seed(1234)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "dat = pd.read_csv('creditcard.csv')\n",
    "\n",
    "print(dat.head())\n",
    "print('\\nThe distribution of the target variable:\\n')\n",
    "dat['Class'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration.\n",
    "\n",
    "Although the context of most of the variables is not known (recall that V1-V28 are factors summarizing the transactional data), we know that V1-V28 are by construction standardized, with a mean of 0 and and a standard deviation of 1. We standardize the Time and Amount variables too.\n",
    "\n",
    "The data exploration and in particular Welch’s t-tests reveal that almost all the factors are significantly associated with the Class variable. The mean of these variables is almost zero in Class 0 and clearly non-zero in Class 1. The Time and Amount variables are also significant. There does not seem to by any reason for variable selection yet. We could drop the Time variable which is probably useless.\n",
    "\n",
    "Some of the factors (and the Amount variable) are quite skewed and have very thin distributions. If we were to apply some other method (say, logistic regression) we could apply some transformations (and probably binning) but XGBoost is insensitive to such deviations from normality.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "\n",
    "dat['Time'] = preprocessing.scale(dat['Time'])\n",
    "dat['Amount'] = preprocessing.scale(dat['Amount'])\n",
    "\n",
    "print('\\nMeans of variables in the two Class categories:\\n')\n",
    "pt = pd.pivot_table(dat, values=dat.columns, columns = 'Class', aggfunc='mean')\n",
    "print(pt.loc[dat.columns])\n",
    "\n",
    "print('\\nP-values of Welch’s t-tests and shape statistics:\\n')\n",
    "for i in range(30):\n",
    "    col_name = dat.columns[i]\n",
    "    t, p_val = stats.ttest_ind(dat.loc[ dat['Class']==0, col_name], dat.loc[ dat['Class']==1, col_name],equal_var=False)  \n",
    "    skewness = dat.loc[:,col_name].skew()\n",
    "    kurtosis = stats.kurtosis(dat.loc[:,col_name])\n",
    "    print('Variable: {:7s}'.format(col_name),end='')    \n",
    "    print('p-value: {:6.3f}  skewness: {:6.3f}  kurtosis: {:6.3f}'.format(p_val, skewness, kurtosis))\n",
    "\n",
    "    \n",
    "fig, axes = plt.subplots(nrows=6, ncols=5,figsize=(10,10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "columns = dat.columns\n",
    "for i in range(30):\n",
    "  axes[i].hist(dat[columns[i]], bins=50,facecolor='b',alpha=0.5)\n",
    "  axes[i].set_title(columns[i])\n",
    "  axes[i].set_xlim([-4., +4.])\n",
    "  plt.setp(axes[i].get_xticklabels(), visible=False) \n",
    "  plt.setp(axes[i].get_yticklabels(), visible=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data partitioning.\n",
    "\n",
    "In this step, we partition the dataset into 40% training, 30% validation and 30% testing. Note the use of the random.shuffle() function from numpy. We also make the corresponding matrices **train**, **valid** and **test** containing predictors only with labels **trainY**, **validY** and **testY**, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Class = dat['Class'].values\n",
    "\n",
    "allIndices = np.arange(len(Class))\n",
    "np.random.shuffle(allIndices) ## shuffle the indices of the observations\n",
    "\n",
    "numTrain = int(round(0.40*len(Class)))\n",
    "numValid = int(round(0.30*len(Class)))\n",
    "numTest = len(Class)-numTrain-numValid\n",
    "\n",
    "inTrain = allIndices[:numTrain]\n",
    "inValid = allIndices[numTrain:(numTrain+numValid)]\n",
    "inTest =  allIndices[(numTrain+numValid):]\n",
    "\n",
    "train = dat.iloc[inTrain,:30]\n",
    "valid= dat.iloc[inValid,:30]\n",
    "test =  dat.iloc[inTest,:30]\n",
    "\n",
    "trainY = Class[inTrain]\n",
    "validY = Class[inValid]\n",
    "testY = Class[inTest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Booster: Fixed parameters.\n",
    "\n",
    "First we create the matrices in the format required by XGBoost with the xgb.DMatrix() function, passing for each dataset the predictors data and the labels. Then we set some fixed parameters. The number of boosting iterations (num_rounds) is set to 20. Normally we would use a larger number, but we want to keep the processing time low for the purposes of this experiment.\n",
    "\n",
    "We initialize the **param dictionary** with silent=1 (no messages). Parameter min_child_weight is set at the default value of 1 because the data is highly unbalanced. This is the minimum weighted number of observations in a child node for further partitioning. The objective is binary classification and the evaluation metric is the Area Under Curve (AUC), the default for binary classification. In a more advanced implementation we could make a customized evaluation function, as described in [XGBoost API](http://xgboost.readthedocs.io/en/latest/python/python_api.html \"XGBoost API\"). The internal random numbers seed is set to a constant for reproducible results (this is not guaranteed though, among other reasons because XGBoost runs in threads).     \n",
    "\n",
    "We are going to **expand the param dictionary** with the parameters in the heuristic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(train, label=trainY)\n",
    "dvalid = xgb.DMatrix(valid, label=validY)\n",
    "dtest = xgb.DMatrix(test, label=testY)\n",
    "\n",
    "## fixed parameters\n",
    "num_rounds=20 # number of boosting iterations\n",
    "\n",
    "param = {'silent':1,\n",
    "         'min_child_weight':1,\n",
    "         'objective':'binary:logistic',\n",
    "         'eval_metric':'auc',\n",
    "         'seed' : 1234}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Booster: Variable parameters \n",
    "\n",
    "In what follows we combine the suggestions from several sources, notably:\n",
    "\n",
    "1. [The official XGBoost documentation](http://xgboost.readthedocs.io/en/latest/parameter.html \"Official XGBoost Guide\") and in particular the [Notes on Parameter Tuning](http://xgboost.readthedocs.io/en/latest/how_to/param_tuning.html \"Notes on Parameter Tuning\")\n",
    "\n",
    "2. [The article \"Complete Guide to Parameter Tuning in XGBoost\" from Analytics Vidhya](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ \"Analytics Vidhya\")  \n",
    "\n",
    "3. [Another Slideshare presentation with title \"Open Source Tools & Data Science Competitions\"](https://www.slideshare.net/odsc/owen-zhangopen-sourcetoolsanddscompetitions1 \"Second Slideshare presentation\")\n",
    "\n",
    "We select several important parameters for the heuristic search:\n",
    "\n",
    "* **max_depth**: the maximum depth of a tree, in [1,∞], with default value 6. This is highly data-dependent. [2] quotes as typical values: 3-10 and [3] advises to start from 6. We choose to explore also larger values and select 5-25 levels in steps of 5.\n",
    "* **subsample**, in (0,1] with default value 1. This is the proportion of the training instances used in trees and smaller values can prevent over-fitting. In [2] values in 0.5-1 are suggested. [3] suggests to leave this at 1. We decide to test values in 0.5-1.0 in steps of 0.1.\n",
    "* **colsample_bytree**, in (0,1] with default value 1. This is the subsample ratio of columns (features) used to construct a tree. In [2] values in 0.5-1 are suggested. The advice in [3] is 0.3-0.5. We will try similar values as with subsample.\n",
    "* **eta** (or **learning_rate**), in [0,1], with default value 0.3. This is the shrinking rate of the feature weights and larger values (but not too high!) can be used to prevent overfitting. A suggestion in [2] is to use values in 0.01-0.2. We can select some values in [0.01,0.4].\n",
    "* **gamma**, in [0, ∞], with default value 0. This is the minimum loss function reduction required for a split. [3] suggests to leave this at 0. We can experiment with values in 0-2 in steps of 0.05.\n",
    "* **scale_pos_weight** which controls the balance of positive and negative weights with default value 1. The advice in [1] is to use the ratio of negative to positive cases which is 524 here, i.e. to put a weight that large to the positive cases. [2] similarly suggests a large value in case of high class imbalance as is the case here. We can try some small values and some larger ones. \n",
    "\n",
    "\n",
    "The **total number of possible combinations** is 43200 and we are only going to test a small fraction of 100 of them, i.e. as many as the number of the heuristic search iterations. \n",
    "\n",
    "We also initialize a dataframe which will hold the results, for later examination and also to avoid repetitions of combinations during the search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "ratio_neg_to_pos = sum(trainY==0)/sum(trainY==1)  ## = 608\n",
    "print('Ratio of negative to positive instances: {:6.1f}'.format(ratio_neg_to_pos))\n",
    "\n",
    "## parameters to be tuned\n",
    "tune_dic = OrderedDict()\n",
    "\n",
    "tune_dic['max_depth']= [5,10,15,20,25] ## maximum tree depth\n",
    "tune_dic['subsample']=[0.5,0.6,0.7,0.8,0.9,1.0] ## proportion of training instances used in trees\n",
    "tune_dic['colsample_bytree']= [0.5,0.6,0.7,0.8,0.9,1.0] ## subsample ratio of columns\n",
    "tune_dic['eta']= [0.01,0.05,0.10,0.20,0.30,0.40]  ## learning rate\n",
    "tune_dic['gamma']= [0.00,0.05,0.10,0.15,0.20]  ## minimum loss function reduction required for a split\n",
    "tune_dic['scale_pos_weight']=[30,40,50,300,400,500,600,700] ## relative weight of positive/negative instances\n",
    "\n",
    "lengths = [len(lst) for lst in tune_dic.values()]\n",
    "\n",
    "combs=1\n",
    "for i in range(len(lengths)):\n",
    "    combs *= lengths[i]\n",
    "print('Total number of combinations: {:16d}'.format(combs))  \n",
    "\n",
    "maxiter=100\n",
    "\n",
    "columns=[*tune_dic.keys()]+['F-Score','Best F-Score']\n",
    "results = pd.DataFrame(index=range(maxiter), columns=columns) ## dataframe to hold training results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for training and performance reporting.\n",
    "\n",
    "Next we define two functions:\n",
    "\n",
    "Function **perf_measures()** accepts some predictions and labels, optionally prints the confusion matrix, and returns the [F-Score](https://en.wikipedia.org/wiki/F1_score \"F-Score\") This is a measure of performance combining [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall \"Precision and recall\") and will guide the heuristic search.\n",
    "\n",
    "Function **do_train()** accepts as parameters:  \n",
    "- the current choice of variable parameters in a dictionary (cur_choice),   \n",
    "- the full dictionary of parameters to be passed to the main XGBoost training routine (param),   \n",
    "- a train dataset in XGBoost format (train),   \n",
    "- a string identifier (train_s),   \n",
    "- its labels (trainY),  \n",
    "- the corresponding arguments for a validation dataset (valid, valid_s, validY),  \n",
    "- and the option to print the confusion matrix (print_conf_matrix). \n",
    "\n",
    "It then trains the model and returns the F-score of the predictions on the validation dataset together with the model. The call to the main train routine **xgb.train()** has the following arguments:   \n",
    "- the full dictionary of the parameters (param),    \n",
    "- the train dataset in XGBoost format (train),  \n",
    "- the number of boosting iterations  (num_boost),  \n",
    "- a watchlist with datasets information to show progress (evals), \n",
    "- the frequency of reporting (verbose_eval), here set to no reporting at all to avoid a very lengthy output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perf_measures(preds, labels, print_conf_matrix=False):\n",
    "    \n",
    "    act_pos=sum(labels==1) ## actual positive\n",
    "    act_neg=len(labels) - act_pos ## actual negative\n",
    "    \n",
    "    pred_pos=sum(1 for i in range(len(preds)) if (preds[i]>=0.5)) ## predicted positive\n",
    "    true_pos=sum(1 for i in range(len(preds)) if (preds[i]>=0.5) & (labels[i]==1)) ## predicted negative\n",
    "    \n",
    "    false_pos=pred_pos - true_pos ## false positive\n",
    "    false_neg=act_pos-true_pos ## false negative\n",
    "    true_neg=act_neg-false_pos ## true negative\n",
    "      \n",
    "    precision = true_pos/pred_pos ## tp/(tp+fp) percentage of correctly classified predicted positives\n",
    "    recall = true_pos/act_pos ## tp/(tp+fn) percentage of positives correctly classified\n",
    "    \n",
    "    f_score = 2*precision*recall/(precision+recall) \n",
    "    \n",
    "    if print_conf_matrix:\n",
    "        print('\\nconfusion matrix')\n",
    "        print('----------------')\n",
    "        print( 'tn:{:6d} fp:{:6d}'.format(true_neg,false_pos))\n",
    "        print( 'fn:{:6d} tp:{:6d}'.format(false_neg,true_pos))\n",
    "    \n",
    "    return(f_score)\n",
    "\n",
    "\n",
    "def do_train(cur_choice, param, train,train_s,trainY,valid,valid_s,validY,print_conf_matrix=False):\n",
    "    ## train with given fixed and variable parameters\n",
    "    ## and report the F-score on the validation dataset\n",
    "    \n",
    "    print('Parameters:')\n",
    "    for (key,value) in cur_choice.items():\n",
    "        print(key,': ',value,' ',end='')\n",
    "        param[key]=value\n",
    "    print('\\n')    \n",
    "    \n",
    "    evallist  = [(train,train_s), (valid,valid_s)]\n",
    "    model = xgb.train( param, train, num_boost_round=num_rounds,\n",
    "                      evals=evallist,verbose_eval=False)    \n",
    "    preds = model.predict(valid)\n",
    "    labels = valid.get_label()\n",
    "      \n",
    "    f_score = perf_measures(preds, labels,print_conf_matrix)\n",
    "    \n",
    "    return(f_score, model)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Producing neighboring combinations.\n",
    "\n",
    "Next we define a function next_choice() which either produces a random combination of the variable parameters (if no current parameters are passed with cur_params) or generates a neighboring combination of the parameters passed in cur_params.   \n",
    "\n",
    "In the second case we first select at random a parameter to be changed. Then:\n",
    "* If this parameter currently has the smallest value, we select the next (larger) one.\n",
    "* If this parameter currently has the largest value, we select the previous (smaller) one.\n",
    "* Otherwise, we select the left (smaller) or right (larger) value randomly.\n",
    "\n",
    "Repetitions are avoided in the function which carries out the heuristic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def next_choice(cur_params=None):\n",
    "    ## returns a random combination of the variable parameters (if cur_params=None)\n",
    "    ## or a random neighboring combination from cur_params\n",
    "    if cur_params:\n",
    "        ## chose parameter to change\n",
    "        ## parameter name and current value\n",
    "        choose_param_name, cur_value = random.choice(list(cur_choice.items())) ## parameter name \n",
    "       \n",
    "        all_values =  list(tune_dic[choose_param_name]) ## all values of selected parameter\n",
    "        cur_index = all_values.index(cur_value) ## current index of selected parameter\n",
    "        \n",
    "        if cur_index==0: ## if it is the first in the range select the second one\n",
    "            next_index=1\n",
    "        elif cur_index==len(all_values)-1: ## if it is the last in the range select the previous one\n",
    "            next_index=len(all_values)-2\n",
    "        else: ## otherwise select the left or right value randomly\n",
    "            direction=np.random.choice([-1,1])\n",
    "            next_index=cur_index + direction\n",
    "\n",
    "        next_params = dict((k,v) for k,v in cur_params.items())\n",
    "        next_params[choose_param_name] = all_values[next_index] ## change the value of the selected parameter\n",
    "        print('selected move: {:10s}: from {:6.2f} to {:6.2f}'.\n",
    "              format(choose_param_name, cur_value, all_values[next_index] ))\n",
    "    else: ## generate a random combination of parameters\n",
    "        next_params=dict()\n",
    "        for i in range(len(tune_dic)):\n",
    "            key = [*tune_dic.keys()][i] \n",
    "            values = [*tune_dic.values()][i]\n",
    "            next_params[key] = np.random.choice(values)\n",
    "    return(next_params)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of the Simulated Annealing algorithm.\n",
    "\n",
    "At each iteration of the Simulated Annealing algorith, one combination of hyper-parameters is selected. The XGBoost algorithm is trained with these parameters and the F-score on the validation set is produced. \n",
    "\n",
    "* If this F-score is better (larger) than the one at the previous iteration, i.e. there is a \"local\" improvement, the combination is accepted as the current combination and a neighbouring combination is selected for the next iteration through a call to the next_choice() function.\n",
    "* Otherwise, i.e. if this F-score is worse (smaller) than the one at the previous iteration and the decline is Δf < 0, the combination is accepted as the current one with probability exp(-beta Δf/T) where beta is a constant and T is the current \"temperature\". The idea is that we start with a high temperature and \"bad\" solutions are easily accepted at first, in the hope of exploring wide areas of the search space. But as the temperature drops, bad solutions are less likely to be accepted and the search becomes more focused.      \n",
    "\n",
    "The temperature starts at a fixed value T0 and is reduced by a factor of alpha < 1 every n number of iterations. Here T0 = 0.40, n=5 and alpha = 0.85. The beta constant is 1.3.  \n",
    "\n",
    "The selection of the parameters of this \"cooling schedule\" can be done easily in MS Excel. In this example we select the average acceptance probabilities for F-Score deterioration of 0.150, 0.075, 0.038, 0.019, 0.009, i.e. starting from 0.150 and dividing by 2. We set these average probabilities to be around 50% during the first, second,...,fifth 20 iterations respectively and we use Excel Solver to find suitable parameters. The Excel file can be found [here](https://github.com/KSpiliop/Fraud_Detection/blob/master/Data/parameters_for_SA.xlsx \"Excel file\").\n",
    "\n",
    "A **warning**: if the number of iterations is not suficiently smaller than the total number of combinations, there may be too many  repetitions and delays. The simple approach for producing combinations implemented here does not address such cases.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.clock()\n",
    "\n",
    "T=0.40\n",
    "best_params = dict() ## initialize dictionary to hold the best parameters\n",
    "\n",
    "best_f_score = -1. ## initialize best f-score\n",
    "prev_f_score = -1. ## initialize previous f-score\n",
    "prev_choice = None ## initialize previous selection of parameters\n",
    "\n",
    "for iter in range(maxiter):\n",
    "    print('\\nIteration = {:5d}  T = {:12.6f}'.format(iter,T))\n",
    "\n",
    "    ## find next selection of parameters not visited before\n",
    "    while True:\n",
    "        cur_choice=next_choice(prev_choice) ## first selection or selection-neighbor of prev_choice\n",
    "        \n",
    "        ## check if selection has already been visited\n",
    "        tmp=abs(results.loc[:,[*cur_choice.keys()]] - list(cur_choice.values()))\n",
    "        tmp=tmp.sum(axis=1)\n",
    "        if any(tmp==0): ## selection has already been visited\n",
    "            print('\\nCombination revisited - searching again')\n",
    "        else:\n",
    "            break ## break out of the while-loop\n",
    "    \n",
    "    \n",
    "    ## train the model and obtain f-score on the validation dataset\n",
    "    f_score,model=do_train(cur_choice, param, dtrain,'train',trainY,dvalid,'valid',validY)\n",
    "    \n",
    "    ## store the parameters\n",
    "    results.loc[iter,[*cur_choice.keys()]]=list(cur_choice.values())\n",
    "    \n",
    "    print('    F-Score: {:6.2f}  previous: {:6.2f}  best so far: {:6.2f}'.format(f_score, prev_f_score, best_f_score))\n",
    " \n",
    "    if f_score > prev_f_score:\n",
    "        print('    Local improvement')\n",
    "        \n",
    "        ## accept this combination as the new starting point\n",
    "        prev_f_score = f_score\n",
    "        prev_choice = cur_choice\n",
    "        \n",
    "        ## update best parameters if the f-score is globally better\n",
    "        if f_score > best_f_score:\n",
    "            \n",
    "            best_f_score = f_score\n",
    "            print('    Global improvement - best f-score updated')\n",
    "            for (key,value) in prev_choice.items():\n",
    "                best_params[key]=value\n",
    "\n",
    "    else: ## f-score is smaller than the previous one\n",
    "        \n",
    "        ## accept this combination as the new starting point with probability exp(-(1.6 x f-score decline)/temperature) \n",
    "        rnd = random.random()\n",
    "        diff = f_score-prev_f_score\n",
    "        thres=np.exp(1.3*diff/T)\n",
    "        if rnd <= thres:\n",
    "            print('    Worse result. F-Score change: {:8.4f}  threshold: {:6.4f}  random number: {:6.4f} -> accepted'.\n",
    "                  format(diff, thres, rnd))\n",
    "            prev_f_score = f_score\n",
    "            prev_choice = cur_choice\n",
    " \n",
    "        else:\n",
    "            ## do not update previous f-score and previous choice\n",
    "            print('    Worse result. F-Score change: {:8.4f}  threshold: {:6.4f}  random number: {:6.4f} -> rejected'.\n",
    "                 format(diff, thres, rnd))\n",
    "    ## store results\n",
    "    results.loc[iter,'F-Score']=f_score\n",
    "    results.loc[iter,'Best F-Score']=best_f_score\n",
    "    if iter % 5 == 0: T=0.85*T  ## reduce temperature every 5 iterations and continue \n",
    "        \n",
    "print('\\n{:6.1f} minutes process time\\n'.format((time.clock() - t0)/60))    \n",
    "\n",
    "print('Best variable parameters found:\\n')\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on the test dataset.\n",
    "\n",
    "The evaluation on the test dataset results to an F-Score of 0.84 which is considered good given the high imbalance in the classes. The run time was 26 minutes on a 64-bit Windows system with 6GB RAM and an Intel Core i3 processor 2.30GHz. \n",
    "\n",
    "The best hyper-parameters found are in the ranges expected to be good according to all sources. One can then proceed this way:\n",
    "* Narrowing the ranges of these hyper-parameters,   \n",
    "* Possibly adding others which are not used here (for example, regularization parameters),\n",
    "* Possibly doing some variable selection on the basis of the variable importance information,\n",
    "* Importantly, combining different models, ensemble-like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\nBest parameters found:\\n')  \n",
    "print(best_params)\n",
    "\n",
    "print('\\nEvaluation on the test dataset\\n')  \n",
    "\n",
    "best_f_score,best_model=do_train(best_params, param, dtrain,'train',trainY,dtest,'test',testY,print_conf_matrix=True)\n",
    "\n",
    "\n",
    "print('\\nF-score on the test dataset: {:6.2f}'.format(best_f_score))\n",
    "\n",
    "\n",
    "f, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, sharey=False, figsize=(8,5))\n",
    "ax1.plot(results['F-Score'])\n",
    "ax2.plot(results['Best F-Score'])\n",
    "ax1.set_xlabel('Iterations',fontsize=11)\n",
    "ax2.set_xlabel('Iterations',fontsize=11)\n",
    "ax1.set_ylabel('F-Score',fontsize=11)\n",
    "ax2.set_ylabel('Best F-Score',fontsize=11)\n",
    "ax1.set_ylim([0.7,0.9])\n",
    "ax2.set_ylim([0.7,0.9])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('\\nVariables importance:\\n')  \n",
    "\n",
    "p = xgb.plot_importance(best_model) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
